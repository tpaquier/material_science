{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4b6481-1990-4f53-b39d-5223f0ed25e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install openpyxl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff804269-c4e4-4004-9fff-72200bd4fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_path = os.getcwd()\n",
    "os.chdir('/home/onyxia/work/material_science/Spetral_clustering')\n",
    "%run rbf.ipynb\n",
    "os.chdir(actual_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1feb54a7-8d02-4eff-8458-3d3605cb58d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_calculated = pd.read_csv('max_calculated.csv')\n",
    "max_elemental = pd.read_csv('max_elemental.csv')\n",
    "list_mxene = pd.read_excel('synthesized-MXenes-MAX.xlsx',sheet_name=0)\n",
    "list_failed = pd.read_excel('synthesized-MXenes-MAX.xlsx', sheet_name=2)\n",
    "n_samples = max_elemental.shape[0]\n",
    "synth_list = pd.unique(list_mxene['MXene'])[:-1]\n",
    "to_drop = list(range(167,173))\n",
    "mx_ene_df = list_mxene.drop(labels = to_drop, axis='index')\n",
    "mx_ene_df = mx_ene_df.drop(['Unnamed: 9','Unnamed: 12','Notes','status','Reference method'],axis=1)\n",
    "max_elemental['class'] = np.zeros(max_elemental.shape[0])\n",
    "parents = mx_ene_df['Parent material'].unique()\n",
    "banned_words = ['+','Mxene','topochemical','reaction', 'or',\n",
    "               'synthesis','MXene','direct']\n",
    "complete_parents = []\n",
    "for i in range(len(parents)):\n",
    "    inter = parents[i].split()\n",
    "    for word in range(len(inter)):\n",
    "        if inter[word] not in banned_words:\n",
    "            complete_parents.append(inter[word])\n",
    "\n",
    "\n",
    "for i in range(max_elemental.shape[0]):\n",
    "    if max_elemental.loc[i,'compound_formula'] in complete_parents:\n",
    "        max_elemental.loc[i,'class'] = 1\n",
    "\n",
    "max_elemental = max_elemental.set_index('compound_formula',drop=True)\n",
    "max_elemental = max_elemental.drop(['M_element', 'A_element', 'X_element'],axis=1)\n",
    "max_calculated = max_calculated.set_index('prettyformula',drop=True)\n",
    "whole_data = max_elemental.copy()\n",
    "x_group = pd.get_dummies(whole_data['X_X_group'],prefix='x_g',dtype=float)\n",
    "a_group = pd.get_dummies(whole_data['A_A_group'],prefix='a_g',dtype=float)\n",
    "m_group = pd.get_dummies(whole_data['M_M_group'],prefix='m_g',dtype=float)\n",
    "whole_data = whole_data.drop(['X_X_group','A_A_group','M_M_group'],axis=1)\n",
    "whole_data = pd.concat([whole_data,x_group,a_group,m_group],axis=1)\n",
    "\n",
    "test_tree = DecisionTreeClassifier().fit(X=whole_data.drop(['class'],axis=1),\n",
    "                                                              y=whole_data['class'])\n",
    "\n",
    "imp_feat = test_tree.feature_importances_\n",
    "names_feat = test_tree.feature_names_in_\n",
    "\n",
    "imp_feat = imp_feat.reshape(-1,1)\n",
    "names_feat = names_feat.reshape(-1,1)\n",
    "test_df = pd.DataFrame(np.hstack((names_feat,imp_feat)))\n",
    "test_df.columns = ['names_feat','imp_feat']\n",
    "test_df = test_df.set_index('names_feat',drop=True)\n",
    "test_df = test_df[test_df['imp_feat'] > 0]\n",
    "\n",
    "diff_z = list(test_df.index)\n",
    "\n",
    "\n",
    "number_of_atoms = np.zeros(n_samples)\n",
    "compteur = 0\n",
    "for element in whole_data.index:\n",
    "    inter = []\n",
    "    for cara in element:\n",
    "        if cara in list(str(1234567890)):\n",
    "            inter.append(cara)\n",
    "    if len(inter) == 1:\n",
    "        number_of_atoms[compteur] = int(inter[0]) + 2\n",
    "    elif len(inter) == 2:\n",
    "        number_of_atoms[compteur] = int(inter[0]) + int(inter[1]) + 1\n",
    "    elif len(inter) == 3:\n",
    "        number_of_atoms[compteur] = int(inter[0]) + int(inter[1]) + int(inter[2])\n",
    "    compteur += 1\n",
    "\n",
    "columns_name = whole_data.drop(['class'],axis=1).columns.copy()\n",
    "normalized = whole_data.drop(['class'],axis=1).to_numpy()/number_of_atoms.reshape(-1,1)\n",
    "\n",
    "data_norm = pd.DataFrame(normalized)\n",
    "data_norm.columns = columns_name\n",
    "data_norm['compound_name'] = whole_data.index\n",
    "data_norm = data_norm.set_index('compound_name',drop=True)\n",
    "\n",
    "data_norm = data_norm.filter(items=list(diff_z),axis=1)\n",
    "data_norm['class'] = whole_data['class'].copy()\n",
    "\n",
    "retained_features = list(test_df.index)\n",
    "\n",
    "for feat in diff_z:\n",
    "    if len(feat) > 5:\n",
    "        retained_features.remove(feat)\n",
    "\n",
    "list_dummies = []\n",
    "\n",
    "for i in retained_features:\n",
    "    if 'M_' in i:\n",
    "        list_dummies.append(i)\n",
    "    elif 'A_' in i:\n",
    "        list_dummies.append(i)\n",
    "    elif 'X_' in i:\n",
    "        list_dummies.append(i)\n",
    "\n",
    "for col in list_dummies:\n",
    "    for row in data_norm.index:\n",
    "        if data_norm.loc[row,col] != 0:\n",
    "            data_norm.loc[row,col] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87a4aac0-5c7e-4541-9ae2-5b193defc14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm['label'] = np.zeros(n_samples)\n",
    "for i in data_norm.index:\n",
    "    if data_norm.loc[i,'class'] == 1:\n",
    "        data_norm.loc[i,'label'] = 1\n",
    "    else:\n",
    "        data_norm.loc[i,'label'] = -1\n",
    "positive_samples = data_norm[data_norm['label'] == 1]\n",
    "unlabelled_samples = data_norm[data_norm['label'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caebd1c2-a700-4ba6-856c-189b0e2b0b7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 101\u001b[0m\n\u001b[1;32m     99\u001b[0m for_loop_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m updated_data\u001b[38;5;241m.\u001b[39mindex:\n\u001b[0;32m--> 101\u001b[0m     updated_data\u001b[38;5;241m.\u001b[39mloc[i,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto_det_b\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrbf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mup_data_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mup_data_np\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfor_loop_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43ml_squared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     for_loop_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    104\u001b[0m to_det_b_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(updated_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto_det_b\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:2485\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2482\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2486\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\n\u001b[1;32m   2488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_of_imp_elem = list(positive_samples.index)\n",
    "predictions_pos = np.zeros(15)\n",
    "proba_pos = np.zeros(15)\n",
    "count_elem = 0\n",
    "for elem_part in list_of_imp_elem:\n",
    "    max_elem_norm = data_norm.drop(elem_part,axis=0)\n",
    "    n_samples = max_elem_norm.shape[0]\n",
    "    #modification to fit with the previous versions\n",
    "    n_cluster = 8\n",
    "    clustering = KMeans(n_clusters=n_cluster).fit(X=max_elem_norm.to_numpy()[:,:-2])\n",
    "    max_elem_norm['cluster'] = clustering.labels_\n",
    "    list_of_ratio = np.zeros(n_cluster)\n",
    "    for i in range(n_cluster):\n",
    "        list_of_ratio[i] = max_elem_norm[max_elem_norm['cluster'] == i]['class'].sum()/max_elem_norm[max_elem_norm['cluster'] == i].shape[0]\n",
    "    #same code as before but here we use the class because it is 0 or 1\n",
    "    positive_cluster = np.argmax(list_of_ratio)\n",
    "        \n",
    "    #we cannot exactly compute the ratios because the classes are so unbalanced that in any cases the number of positive\n",
    "    #instances will be very small compared to the ones of unlabelled instances\n",
    "    \n",
    "    list_of_dist = np.zeros(n_cluster)\n",
    "    for i in range(n_cluster):\n",
    "        list_of_dist[i] = np.linalg.norm(clustering.cluster_centers_[positive_cluster,:] - clustering.cluster_centers_[i,:])\n",
    "    \n",
    "    negative_cluster = np.argmax(list_of_dist)\n",
    "    df_unlab_pop = max_elem_norm[max_elem_norm['label'] == -1]\n",
    "    list_of_pop = pd.DataFrame(df_unlab_pop.groupby('cluster')['label'].count())\n",
    "    list_of_pop.columns = ['pop']\n",
    "    list_of_pop['dist'] = list_of_dist #distance to the positive cluster\n",
    "    list_of_pop = list_of_pop.sort_values('dist',ascending=False)\n",
    "    list_of_pop['cumsum'] = np.cumsum(list_of_pop['pop'])\n",
    "    reliable_positives = max_elem_norm[max_elem_norm['label'] == 1]\n",
    "    n_positives = reliable_positives.shape[0]\n",
    "    last_step = np.where(np.array(list_of_pop['cumsum'])>n_positives*10)[0][0]\n",
    "    index_ordered_distance = list(list_of_pop.index)\n",
    "    if last_step == 0:\n",
    "        reliable_negatives = max_elem_norm[max_elem_norm['cluster'] == negative_cluster]\n",
    "        reliable_negatives = reliable_negatives[reliable_negatives['label'] == -1]\n",
    "    else:\n",
    "        compteur=0\n",
    "        reliable_negatives = max_elem_norm[max_elem_norm['cluster'] == negative_cluster]\n",
    "        reliable_negatives = reliable_negatives[reliable_negatives['label'] == -1]\n",
    "        while compteur<last_step:\n",
    "            interm_negatives = max_elem_norm[max_elem_norm['cluster'] == index_ordered_distance[compteur+1]]\n",
    "            interm_negatives = interm_negatives[interm_negatives['label'] == -1]\n",
    "            reliable_negatives = pd.concat([reliable_negatives,interm_negatives])\n",
    "            compteur += 1\n",
    "        del interm_negatives, compteur\n",
    "    \n",
    "    reliable_negatives = reliable_negatives.head(n=n_positives*10)\n",
    "    \n",
    "    #Step of initialization of labels\n",
    "    train_clf_data = pd.concat([reliable_positives,reliable_negatives])\n",
    "    index_of_labels = list(train_clf_data.index)\n",
    "    unlabelled_data = max_elem_norm.drop(labels=index_of_labels,axis=0)\n",
    "    index_of_unlabelled = list(unlabelled_data.index)\n",
    "    first_step_clf = SVC().fit(X=train_clf_data.drop(['class','label','cluster'],axis=1).to_numpy(),\n",
    "                              y=train_clf_data['label'].to_numpy())\n",
    "    unlabelled_data['relab'] = first_step_clf.predict(unlabelled_data.drop(['class','label','cluster'],axis=1).to_numpy())\n",
    "\n",
    "    gamma = 1\n",
    "    good_ratio = 1/10\n",
    "    max_iter = 10\n",
    "    compteur = 0\n",
    "    train_clf_data['relab'] = train_clf_data['label'].copy()\n",
    "    updated_data = pd.concat([train_clf_data,unlabelled_data])\n",
    "    up_data_np = updated_data.to_numpy()[:,:-4].copy()\n",
    "    positive_index_list = list(max_elem_norm[max_elem_norm['class'] == 1].index)\n",
    "\n",
    "    right_side = np.vstack((np.zeros(1).reshape(1,1),np.ones(n_samples).reshape(n_samples,1))) #its for the \n",
    "    #computation of the matrix to det the coeffs so put it here to avoid doing it each time\n",
    "    while compteur<max_iter:\n",
    "        compteur += 1\n",
    "        labels = updated_data['relab'].to_numpy().reshape(1,-1)\n",
    "        first_row = np.hstack((np.array(0).reshape(1,1),(-1)*labels))\n",
    "        \n",
    "        #computation of omega and the coefficients\n",
    "        omega = np.zeros((n_samples,n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for k in range(i,n_samples):\n",
    "                omega[i,k] = rbf(x=up_data_np[i,:],y=up_data_np[k,:],l_squared=50)*labels[0,i]*labels[0,k]\n",
    "                omega[k,i] = omega[i,k]\n",
    "            omega[i,i] = 1\n",
    "    \n",
    "        bot_right = omega + gamma*np.eye(n_samples)\n",
    "        bot = np.hstack((updated_data['relab'].to_numpy().reshape(n_samples,1), bot_right))\n",
    "        whole_mat = np.vstack((first_row, bot))\n",
    "        \n",
    "        del bot_right, bot, first_row\n",
    "    \n",
    "        coeffs = np.linalg.solve(a=whole_mat,b=right_side)\n",
    "\n",
    "\n",
    "        alpha = coeffs[1:]\n",
    "    \n",
    "        #once we have the coefficients, we can compute the labels of the unlabelled instances\n",
    "    \n",
    "        updated_data['to_det_b'] = np.zeros(n_samples)\n",
    "        for_loop_count = 0\n",
    "        for i in updated_data.index:\n",
    "            updated_data.loc[i,'to_det_b'] = np.sum(alpha*labels*rbf(x=up_data_np,y=up_data_np[for_loop_count,:],l_squared=50))\n",
    "            for_loop_count += 1\n",
    "    \n",
    "        to_det_b_arr = np.array(updated_data['to_det_b']).copy()\n",
    "        b = np.sort(to_det_b_arr)[int((1-good_ratio)*n_samples)]\n",
    "        \n",
    "        updated_data['check_array'] = np.zeros(n_samples)\n",
    "        count_diff = 0\n",
    "        \n",
    "        for i in updated_data.index:\n",
    "            if i in positive_index_list:\n",
    "                updated_data.loc[i,'check_array'] = 1\n",
    "            else:\n",
    "                updated_data.loc[i,'check_array'] = np.sign(updated_data.loc[i,'to_det_b']-b)\n",
    "                if updated_data.loc[i,'check_array'] != updated_data.loc[i,'relab']:\n",
    "                    count_diff += 1\n",
    "        \n",
    "        if count_diff == 0:\n",
    "            break\n",
    "        else:\n",
    "            updated_data['relab'] = updated_data['check_array'].copy()\n",
    "\n",
    "\n",
    "    predictions_pos[count_elem] = np.sign(np.sum(alpha*labels*rbf(x=up_data_np,\n",
    "                                                                  y=data_norm.loc[elem_part,:].to_numpy().reshape(1,-1)[:,:-2],\n",
    "                                                                  l_squared=50))-b)\n",
    "                                 \n",
    "    positives_whole = 0\n",
    "    for i in updated_data.index:\n",
    "        if updated_data.loc[i,'relab'] == 1:\n",
    "            positives_whole += 1\n",
    "    positives_whole += -14\n",
    "    #-14 because at each iteration we fix the known positives as positives, so there are \n",
    "    #14 predictions that are not truely predictions\n",
    "    proba_pos[count_elem] = positives_whole/(updated_data.shape[0]-14)\n",
    "    count_elem += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866136ab-5c86-45f8-b0bc-021a3233bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b8b17-e310-4473-a42a-3a3946eb5e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_data[updated_data['check_array']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3156f2-280a-4610-8b19-7a3a5912f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7d94d-8aad-4620-b692-c58fdadf95f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(predictions_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a8610-db5d-4faa-b9ca-4500f573fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_test = updated_data.copy()\n",
    "det_sign_array = updated_data['to_det_b'].to_numpy()\n",
    "df_copy_test['final_relab'] = np.sign(det_sign_array - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa36751-eb7b-4896-a238-8cd740c22aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_test[df_copy_test['final_relab'] == 1].sum()['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f5c60-673c-42d3-8168-68e1d72b20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = df_copy_test[df_copy_test['final_relab']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59548b-c849-463c-9962-7e3f05d2601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c571d-94f1-4139-aa1a-ab0f8cc07a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interm = whole_data.merge(max_calculated,how='left',left_index=True,\n",
    "                          right_index=True)\n",
    "to_plot = to_plot.merge(max_calculated,how='left',left_index=True,\n",
    "                          right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2dda6-4c25-4e16-a069-93f1fcf77750",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives_plot = to_plot[to_plot['class']==1]\n",
    "predicted_positives = to_plot.drop(list(true_positives_plot.index),axis=0)\n",
    "predicted_negatives = interm.drop(list(predicted_positives.index),axis=0)\n",
    "false_negative = predicted_negatives[predicted_negatives['class']==1]\n",
    "predicted_negatives = predicted_negatives.drop(list(false_negative.index),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfda615-f1c9-489c-ae8d-8545b1df5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.grid(alpha=0.5)\n",
    "ax.plot(predicted_negatives['dEf'],predicted_negatives['dH'],'o',color='lightgrey',\n",
    "        alpha=0.5,label='predicted negatives')\n",
    "ax.plot(false_negative['dEf'],false_negative['dH'],'x',color='red',\n",
    "        label='false negative')\n",
    "ax.plot(predicted_positives['dEf'],predicted_positives['dH'],'x',color='gold',\n",
    "        label='predicted positives')\n",
    "ax.plot(true_positives_plot['dEf'],true_positives_plot['dH'],'x',color='mediumblue',\n",
    "        label='true positive')\n",
    "ax.plot(np.linspace(-2000,1000,2,endpoint=True),np.zeros(2),'k--',alpha=0.3)\n",
    "ax.plot(np.zeros(2),np.linspace(-200,1200,2,endpoint=True),'k--',alpha=0.3)\n",
    "ax.fill_between(np.linspace(-2000,0,2,endpoint=True),y1=-200,y2=0,color='forestgreen',\n",
    "                alpha=0.5)\n",
    "plt.legend()\n",
    "ax.set_xlabel('formation energy')\n",
    "ax.set_ylabel('formation enthalpy')\n",
    "ax.set_title('r=1/10, l_squared=50')\n",
    "plt.suptitle('correct and incorrect classif kfold cross v')\n",
    "plt.savefig('pred_elem_only_kflod_10_50_modif_init.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0b64d-c7f1-4b2e-a3f7-a13c03f27dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(predicted_positives.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39456443-ff11-48a7-af41-7703c87c4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e1ec9-0cb0-4fed-be4d-66cad74624cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_test.sort_values(by='to_det_b', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e3ac6c-f2a3-486a-8935-f62142af1f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd3428-f2c4-4810-bb4f-8f9461bdca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e25233-33aa-4861-a93e-64a618636c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
