{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4b6481-1990-4f53-b39d-5223f0ed25e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install openpyxl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff804269-c4e4-4004-9fff-72200bd4fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_path = os.getcwd()\n",
    "os.chdir('/home/onyxia/work/material_science/Spetral_clustering')\n",
    "%run rbf.ipynb\n",
    "os.chdir(actual_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1feb54a7-8d02-4eff-8458-3d3605cb58d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_calculated = pd.read_csv('max_calculated.csv')\n",
    "max_elemental = pd.read_csv('max_elemental.csv')\n",
    "list_mxene = pd.read_excel('synthesized-MXenes-MAX.xlsx',sheet_name=0)\n",
    "list_failed = pd.read_excel('synthesized-MXenes-MAX.xlsx', sheet_name=2)\n",
    "n_samples = max_elemental.shape[0]\n",
    "synth_list = pd.unique(list_mxene['MXene'])[:-1]\n",
    "to_drop = list(range(167,173))\n",
    "mx_ene_df = list_mxene.drop(labels = to_drop, axis='index')\n",
    "mx_ene_df = mx_ene_df.drop(['Unnamed: 9','Unnamed: 12','Notes','status','Reference method'],axis=1)\n",
    "max_elemental['class'] = np.zeros(max_elemental.shape[0])\n",
    "parents = mx_ene_df['Parent material'].unique()\n",
    "banned_words = ['+','Mxene','topochemical','reaction', 'or',\n",
    "               'synthesis','MXene','direct']\n",
    "complete_parents = []\n",
    "for i in range(len(parents)):\n",
    "    inter = parents[i].split()\n",
    "    for word in range(len(inter)):\n",
    "        if inter[word] not in banned_words:\n",
    "            complete_parents.append(inter[word])\n",
    "\n",
    "\n",
    "for i in range(max_elemental.shape[0]):\n",
    "    if max_elemental.loc[i,'compound_formula'] in complete_parents:\n",
    "        max_elemental.loc[i,'class'] = 1\n",
    "\n",
    "max_elemental = max_elemental.set_index('compound_formula',drop=True)\n",
    "max_elemental = max_elemental.drop(['M_element', 'A_element', 'X_element'],axis=1)\n",
    "max_calculated = max_calculated.set_index('prettyformula',drop=True)\n",
    "whole_data = max_elemental.merge(max_calculated,how='inner',\n",
    "                                 left_index=True,right_index=True)\n",
    "whole_data = whole_data.drop(['PU_label','year'],axis=1)\n",
    "M_elements = pd.get_dummies(whole_data['M'],prefix='M',dtype=float)\n",
    "A_elements = pd.get_dummies(whole_data['A'],prefix='A',dtype=float)\n",
    "X_elements = pd.get_dummies(whole_data['X'],prefix='X',dtype=float)\n",
    "whole_data = whole_data.drop(['M','A','X'],axis=1)\n",
    "x_group = pd.get_dummies(whole_data['X_X_group'],prefix='x_g',dtype=float)\n",
    "a_group = pd.get_dummies(whole_data['A_A_group'],prefix='a_g',dtype=float)\n",
    "m_group = pd.get_dummies(whole_data['M_M_group'],prefix='m_g',dtype=float)\n",
    "whole_data = whole_data.drop(['X_X_group','A_A_group','M_M_group'],axis=1)\n",
    "whole_data = pd.concat([whole_data,M_elements,A_elements,X_elements,x_group,\n",
    "                       a_group,m_group],axis=1)\n",
    "\n",
    "test_tree = DecisionTreeClassifier().fit(X=whole_data.drop(['class'],axis=1),\n",
    "                                                              y=whole_data['class'])\n",
    "\n",
    "imp_feat = test_tree.feature_importances_\n",
    "names_feat = test_tree.feature_names_in_\n",
    "\n",
    "imp_feat = imp_feat.reshape(-1,1)\n",
    "names_feat = names_feat.reshape(-1,1)\n",
    "test_df = pd.DataFrame(np.hstack((names_feat,imp_feat)))\n",
    "test_df.columns = ['names_feat','imp_feat']\n",
    "test_df = test_df.set_index('names_feat',drop=True)\n",
    "test_df = test_df[test_df['imp_feat'] > 0]\n",
    "\n",
    "diff_z = list(test_df.index)\n",
    "\n",
    "\n",
    "number_of_atoms = np.zeros(n_samples)\n",
    "compteur = 0\n",
    "for element in whole_data.index:\n",
    "    inter = []\n",
    "    for cara in element:\n",
    "        if cara in list(str(1234567890)):\n",
    "            inter.append(cara)\n",
    "    if len(inter) == 1:\n",
    "        number_of_atoms[compteur] = int(inter[0]) + 2\n",
    "    elif len(inter) == 2:\n",
    "        number_of_atoms[compteur] = int(inter[0]) + int(inter[1]) + 1\n",
    "    elif len(inter) == 3:\n",
    "        number_of_atoms[compteur] = int(inter[0]) + int(inter[1]) + int(inter[2])\n",
    "    compteur += 1\n",
    "\n",
    "columns_name = whole_data.drop(['class'],axis=1).columns.copy()\n",
    "normalized = whole_data.drop(['class'],axis=1).to_numpy()/number_of_atoms.reshape(-1,1)\n",
    "\n",
    "data_norm = pd.DataFrame(normalized)\n",
    "data_norm.columns = columns_name\n",
    "data_norm['compound_name'] = whole_data.index\n",
    "data_norm = data_norm.set_index('compound_name',drop=True)\n",
    "\n",
    "data_norm = data_norm.filter(items=list(diff_z),axis=1)\n",
    "data_norm['class'] = whole_data['class'].copy()\n",
    "\n",
    "retained_features = list(test_df.index)\n",
    "\n",
    "for feat in diff_z:\n",
    "    if len(feat) > 5:\n",
    "        retained_features.remove(feat)\n",
    "\n",
    "list_dummies = []\n",
    "\n",
    "for i in retained_features:\n",
    "    if 'M_' in i:\n",
    "        list_dummies.append(i)\n",
    "    elif 'A_' in i:\n",
    "        list_dummies.append(i)\n",
    "    elif 'X_' in i:\n",
    "        list_dummies.append(i)\n",
    "\n",
    "for col in list_dummies:\n",
    "    for row in data_norm.index:\n",
    "        if data_norm.loc[row,col] != 0:\n",
    "            data_norm.loc[row,col] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87a4aac0-5c7e-4541-9ae2-5b193defc14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm['label'] = np.zeros(n_samples)\n",
    "for i in data_norm.index:\n",
    "    if data_norm.loc[i,'class'] == 1:\n",
    "        data_norm.loc[i,'label'] = 1\n",
    "    else:\n",
    "        data_norm.loc[i,'label'] = -1\n",
    "positive_samples = data_norm[data_norm['label'] == 1]\n",
    "unlabelled_samples = data_norm[data_norm['label'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caebd1c2-a700-4ba6-856c-189b0e2b0b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_imp_elem = list(positive_samples.index)\n",
    "predictions_pos = np.zeros(15)\n",
    "proba_pos = np.zeros(15)\n",
    "count_elem = 0\n",
    "for elem_part in list_of_imp_elem:\n",
    "    max_elem_norm = data_norm.drop(elem_part,axis=0)\n",
    "    n_samples = max_elem_norm.shape[0]\n",
    "    #modification to fit with the previous versions\n",
    "    n_cluster = 8\n",
    "    clustering = KMeans().fit(X=max_elem_norm.to_numpy()[:,:-2])\n",
    "    max_elem_norm['cluster'] = clustering.labels_\n",
    "    list_of_ratio = np.zeros(n_cluster)\n",
    "    for i in range(n_cluster):\n",
    "        list_of_ratio[i] = max_elem_norm[max_elem_norm['cluster'] == i]['class'].sum()/max_elem_norm[max_elem_norm['cluster'] == i].shape[0]\n",
    "    #same code as before but here we use the class because it is 0 or 1\n",
    "    positive_cluster = np.argmax(list_of_ratio)\n",
    "        \n",
    "    #we cannot exactly compute the ratios because the classes are so unbalanced that in any cases the number of positive\n",
    "    #instances will be very small compared to the ones of unlabelled instances\n",
    "    \n",
    "    list_of_dist = np.zeros(n_cluster)\n",
    "    for i in range(n_cluster):\n",
    "        list_of_dist[i] = np.linalg.norm(clustering.cluster_centers_[positive_cluster,:] - clustering.cluster_centers_[i,:])\n",
    "    \n",
    "    negative_cluster = np.argmax(list_of_dist)\n",
    "    df_unlab_pop = max_elem_norm[max_elem_norm['label'] == -1]\n",
    "    list_of_pop = pd.DataFrame(df_unlab_pop.groupby('cluster')['label'].count())\n",
    "    list_of_pop.columns = ['pop']\n",
    "    list_of_pop['dist'] = list_of_dist #distance to the positive cluster\n",
    "    list_of_pop = list_of_pop.sort_values('dist',ascending=False)\n",
    "    list_of_pop['cumsum'] = np.cumsum(list_of_pop['pop'])\n",
    "    reliable_positives = max_elem_norm[max_elem_norm['label'] == 1]\n",
    "    n_positives = reliable_positives.shape[0]\n",
    "    last_step = np.where(np.array(list_of_pop['cumsum'])>n_positives)[0][0]\n",
    "    index_ordered_distance = list(list_of_pop.index)\n",
    "    if last_step == 0:\n",
    "        reliable_negatives = max_elem_norm[max_elem_norm['cluster'] == negative_cluster]\n",
    "        reliable_negatives = reliable_negatives[reliable_negatives['label'] == -1]\n",
    "    else:\n",
    "        compteur=0\n",
    "        reliable_negatives = max_elem_norm[max_elem_norm['cluster'] == negative_cluster]\n",
    "        reliable_negatives = reliable_negatives[reliable_negatives['label'] == -1]\n",
    "        while compteur<last_step:\n",
    "            interm_negatives = max_elem_norm[max_elem_norm['cluster'] == index_ordered_distance[compteur+1]]\n",
    "            interm_negatives = interm_negatives[interm_negatives['label'] == -1]\n",
    "            reliable_negatives = pd.concat([reliable_negatives,interm_negatives])\n",
    "            compteur += 1\n",
    "        del interm_negatives, compteur\n",
    "    \n",
    "    reliable_negatives = reliable_negatives.head(n=n_positives)\n",
    "    \n",
    "    #Step of initialization of labels\n",
    "    train_clf_data = pd.concat([reliable_positives,reliable_negatives])\n",
    "    index_of_labels = list(train_clf_data.index)\n",
    "    unlabelled_data = max_elem_norm.drop(labels=index_of_labels,axis=0)\n",
    "    index_of_unlabelled = list(unlabelled_data.index)\n",
    "    first_step_clf = SVC().fit(X=train_clf_data.drop(['class','label','cluster'],axis=1).to_numpy(),\n",
    "                              y=train_clf_data['label'].to_numpy())\n",
    "    unlabelled_data['relab'] = first_step_clf.predict(unlabelled_data.drop(['class','label','cluster'],axis=1).to_numpy())\n",
    "\n",
    "    gamma = 1\n",
    "    good_ratio = 1/2\n",
    "    max_iter = 10\n",
    "    compteur = 0\n",
    "    train_clf_data['relab'] = train_clf_data['label'].copy()\n",
    "    updated_data = pd.concat([train_clf_data,unlabelled_data])\n",
    "    up_data_np = updated_data.to_numpy()[:,:-4].copy()\n",
    "    positive_index_list = list(max_elem_norm[max_elem_norm['class'] == 1].index)\n",
    "\n",
    "    right_side = np.vstack((np.zeros(1).reshape(1,1),np.ones(n_samples).reshape(n_samples,1))) #its for the \n",
    "    #computation of the matrix to det the coeffs so put it here to avoid doing it each time\n",
    "    while compteur<max_iter:\n",
    "        compteur += 1\n",
    "        labels = updated_data['relab'].to_numpy().reshape(1,-1)\n",
    "        first_row = np.hstack((np.array(0).reshape(1,1),labels))\n",
    "        \n",
    "        #computation of omega and the coefficients\n",
    "        omega = np.zeros((n_samples,n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for k in range(i,n_samples):\n",
    "                omega[i,k] = rbf(x=up_data_np[i,:],y=up_data_np[k,:],l_squared=10)*labels[0,i]*labels[0,k]\n",
    "                omega[k,i] = omega[i,k]\n",
    "            omega[i,i] = 1\n",
    "    \n",
    "        bot_right = omega + gamma*np.eye(n_samples)\n",
    "        bot = np.hstack((updated_data['relab'].to_numpy().reshape(n_samples,1), bot_right))\n",
    "        whole_mat = np.vstack((first_row, bot))\n",
    "        \n",
    "        del bot_right, bot, first_row\n",
    "    \n",
    "        coeffs = np.linalg.solve(a=whole_mat,b=right_side)\n",
    "\n",
    "\n",
    "        alpha = coeffs[1:]\n",
    "    \n",
    "        #once we have the coefficients, we can compute the labels of the unlabelled instances\n",
    "    \n",
    "        updated_data['to_det_b'] = np.zeros(n_samples)\n",
    "        for_loop_count = 0\n",
    "        for i in updated_data.index:\n",
    "            updated_data.loc[i,'to_det_b'] = np.sum(alpha*labels*rbf(x=up_data_np,y=up_data_np[for_loop_count,:],l_squared=10))\n",
    "            for_loop_count += 1\n",
    "    \n",
    "        to_det_b_arr = np.array(updated_data['to_det_b']).copy()\n",
    "        b = np.sort(to_det_b_arr)[int(good_ratio*n_samples)]\n",
    "        \n",
    "        updated_data['check_array'] = np.zeros(n_samples)\n",
    "        count_diff = 0\n",
    "        \n",
    "        for i in updated_data.index:\n",
    "            if i in positive_index_list:\n",
    "                updated_data.loc[i,'check_array'] = 1\n",
    "            else:\n",
    "                updated_data.loc[i,'check_array'] = np.sign(updated_data.loc[i,'to_det_b']-b)\n",
    "                if updated_data.loc[i,'check_array'] != updated_data.loc[i,'relab']:\n",
    "                    count_diff += 1\n",
    "        \n",
    "        if count_diff == 0:\n",
    "            break\n",
    "        else:\n",
    "            updated_data['relab'] = updated_data['check_array'].copy()\n",
    "\n",
    "\n",
    "    predictions_pos[count_elem] = np.sign(np.sum(alpha*labels*rbf(x=up_data_np,\n",
    "                                                                  y=data_norm.loc[elem_part,:].to_numpy().reshape(1,-1)[:,:-2],\n",
    "                                                                  l_squared=10))-b)\n",
    "                                 \n",
    "    positives_whole = 0\n",
    "    for i in updated_data.index:\n",
    "        if updated_data.loc[i,'relab'] == 1:\n",
    "            positives_whole += 1\n",
    "    positives_whole += -14\n",
    "    #-14 because at each iteration we fix the known positives as positives, so there are \n",
    "    #14 predictions that are not truely predictions\n",
    "    proba_pos[count_elem] = positives_whole/(updated_data.shape[0]-14)\n",
    "    count_elem += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "866136ab-5c86-45f8-b0bc-021a3233bbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "       -1., -1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7234da86-7770-49f1-95f2-929ebbf6f284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
